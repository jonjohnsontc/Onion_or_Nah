{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01_gathering_data\n",
    "\n",
    "Within these notebooks, I'll build a model that will be able to differentiate between Satirical and Sensational news content. This content should be reviewed alongside my upcoming presentation, whose slides can be viewed [here](https://docs.google.com/presentation/d/1nEYjhchUErCDQR8fqr9AunZtu1Rai8-R14BtKmHf9_Q/edit?usp=sharing).\n",
    "\n",
    "**Description:** Querying Pushshift API for Subreddit data, and then saving it for EDA and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Pushshift API**\n",
    "\n",
    "I had tried using Reddit's API directly, though I found Pushshift much easier to use, as you can request virtually as many pages as you'd like, within of the 60 requests/min max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = requests.get('https://api.pushshift.io/reddit/search/submission/?subreddit=nottheonion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_js = tr.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'PM_ME_YOUR_BITCOlN',\n",
       " 'author_flair_css_class': None,\n",
       " 'author_flair_richtext': [],\n",
       " 'author_flair_text': None,\n",
       " 'author_flair_type': 'text',\n",
       " 'author_fullname': 't2_n0y4u8n',\n",
       " 'can_mod_post': False,\n",
       " 'contest_mode': False,\n",
       " 'created_utc': 1536545998,\n",
       " 'domain': 'longisland.news12.com',\n",
       " 'full_link': 'https://www.reddit.com/r/nottheonion/comments/9eis00/residents_crack_pipe_dispenser_found_near_busy/',\n",
       " 'id': '9eis00',\n",
       " 'is_crosspostable': False,\n",
       " 'is_meta': False,\n",
       " 'is_original_content': False,\n",
       " 'is_reddit_media_domain': False,\n",
       " 'is_self': False,\n",
       " 'is_video': False,\n",
       " 'link_flair_background_color': '',\n",
       " 'link_flair_richtext': [],\n",
       " 'link_flair_text_color': 'dark',\n",
       " 'link_flair_type': 'text',\n",
       " 'locked': False,\n",
       " 'media_only': False,\n",
       " 'no_follow': True,\n",
       " 'num_comments': 2,\n",
       " 'num_crossposts': 0,\n",
       " 'over_18': False,\n",
       " 'parent_whitelist_status': 'all_ads',\n",
       " 'permalink': '/r/nottheonion/comments/9eis00/residents_crack_pipe_dispenser_found_near_busy/',\n",
       " 'pinned': False,\n",
       " 'post_hint': 'link',\n",
       " 'preview': {'enabled': False,\n",
       "  'images': [{'id': '_S8f8qcwjGLyyj-bsoZoK0fOPAjx9Esnz_7pFJ544Qw',\n",
       "    'resolutions': [{'height': 60,\n",
       "      'url': 'https://i.redditmedia.com/-mSuDzogioeAcFbcOjAGtsUytU7Qd-fT4x0L4khjL_o.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=108&amp;s=4194d65d148668ae85e54a9b68b4f2cb',\n",
       "      'width': 108},\n",
       "     {'height': 121,\n",
       "      'url': 'https://i.redditmedia.com/-mSuDzogioeAcFbcOjAGtsUytU7Qd-fT4x0L4khjL_o.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=216&amp;s=844bca11c33db02031fe3916e375f222',\n",
       "      'width': 216},\n",
       "     {'height': 180,\n",
       "      'url': 'https://i.redditmedia.com/-mSuDzogioeAcFbcOjAGtsUytU7Qd-fT4x0L4khjL_o.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=320&amp;s=ef1f2e08b8f065b9627361fcf7dcb242',\n",
       "      'width': 320},\n",
       "     {'height': 360,\n",
       "      'url': 'https://i.redditmedia.com/-mSuDzogioeAcFbcOjAGtsUytU7Qd-fT4x0L4khjL_o.jpg?fit=crop&amp;crop=faces%2Centropy&amp;arh=2&amp;w=640&amp;s=e2efc9f5eb90a30e576a1d994627d7c5',\n",
       "      'width': 640}],\n",
       "    'source': {'height': 360,\n",
       "     'url': 'https://i.redditmedia.com/-mSuDzogioeAcFbcOjAGtsUytU7Qd-fT4x0L4khjL_o.jpg?s=b3a58d9dce3318d6f84a47f298c3b205',\n",
       "     'width': 640},\n",
       "    'variants': {}}]},\n",
       " 'pwls': 6,\n",
       " 'retrieved_on': 1536545999,\n",
       " 'score': 1,\n",
       " 'selftext': '',\n",
       " 'send_replies': True,\n",
       " 'spoiler': False,\n",
       " 'stickied': False,\n",
       " 'subreddit': 'nottheonion',\n",
       " 'subreddit_id': 't5_2qnts',\n",
       " 'subreddit_subscribers': 13989902,\n",
       " 'subreddit_type': 'public',\n",
       " 'thumbnail': 'https://b.thumbs.redditmedia.com/01bRXXh7RW7yXdkXRKLGJDLffPN3pnexBnyj8HnXlOQ.jpg',\n",
       " 'thumbnail_height': 78,\n",
       " 'thumbnail_width': 140,\n",
       " 'title': 'Residents: Crack pipe dispenser found near busy shopping center, bus stop',\n",
       " 'url': 'http://longisland.news12.com/story/39055695/residents-crack-pipe-dispenser-found-near-busy-shopping-center-bus-stop',\n",
       " 'whitelist_status': 'all_ads',\n",
       " 'wls': 6}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_js['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Residents: Crack pipe dispenser found near busy shopping center, bus stop'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_js['data'][0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_list = []\n",
    "post_list.extend(tr_js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work well, I'll now try to programatically grab posts from each subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gathering Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off of the `total_results` key within pushshift API, the two subreddits that I'm looking at comparing contain the following number of posts:\n",
    "\n",
    "- /r/theonion: 7124\n",
    "- /r/notheonion: 115527\n",
    "\n",
    "Given the size of both subreddits, I'll look to gather 5000 posts from each to analyze. That should give me enough data for my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []\n",
    "before = None\n",
    "for i in range(200):\n",
    "    if before == None:\n",
    "        params = {}\n",
    "    else:\n",
    "        params = {'before': before}\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?subreddit=nottheonion'\n",
    "    res = requests.get(url, params=params)\n",
    "    if res.status_code == 200:\n",
    "        the_json = res.json()\n",
    "        posts.extend(the_json['data'])\n",
    "        before = the_json['data'][-1]['created_utc']\n",
    "    else:\n",
    "        print(res.status_code)\n",
    "        break\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('../data/nottheonion_ps_20180830.json', 'w') as fout:\n",
    "#     json.dump(posts, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separating Titles from JSON and Saving into List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = []\n",
    "c = 0\n",
    "for i in range(len(posts)):\n",
    "    title_list.append(posts[c]['title'])\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4093"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(title_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm a little disappointed at the number of duplicate posts that I found within the first 5000 that I've pulled. Nonetheless, over 4,000 posts still seems acceptable for generating a model. I'll consider grabbing more if I'm able to find a much larger portion of originals with The Onion subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sub'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {0:'Title'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Rid of Duplicate Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Working on /r/TheOnion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_o = []\n",
    "before = None\n",
    "for i in range(200):\n",
    "    if before == None:\n",
    "        params = {}\n",
    "    else:\n",
    "        params = {'before': before}\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?subreddit=theonion'\n",
    "    res = requests.get(url, params=params)\n",
    "    if res.status_code == 200:\n",
    "        the_json = res.json()\n",
    "        posts_o.extend(the_json['data'])\n",
    "        before = the_json['data'][-1]['created_utc']\n",
    "    else:\n",
    "        print(res.status_code)\n",
    "        break\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('../data/theonion_ps_20180830.json', 'w') as fout:\n",
    "#     json.dump(posts_o, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list_o = []\n",
    "c = 0\n",
    "for i in range(len(posts_o)):\n",
    "    title_list_o.append(posts_o[c]['title'])\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(title_list_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4644"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(title_list_o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm a bit surprised that I had slightly better luck in finding original posts with The Onion subreddit. The difference between them isn't large enough to justify grabbing more posts from Not the Onion, however, so I'll move forward to pre-processing my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o = pd.DataFrame(title_list_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o['sub'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o.rename(columns = {0:'Title'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting Rid of Duplicate Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4093 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      "Title    4093 non-null object\n",
      "sub      4093 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 95.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4644 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      "Title    4644 non-null object\n",
      "sub      4644 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 108.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info(), df_o.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving Title Lists as a `pickle`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/nto_t.pkl\", 'wb') as nto_t:\n",
    "#     pickle.dump(df, nto_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/to_t.pkl\", 'wb') as to_t:\n",
    "#     pickle.dump(df_o, to_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go to: 02_preprocessing**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
